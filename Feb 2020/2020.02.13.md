# [6-1주차 : 미적분/최적화 part 3. 최적화 기초, 제한조건](https://www.notion.so/6-1-part-3-95f192e1fe12458eb5062ef55c6abb3b)

(주) 5.1~2 최적화 기초, 제한조건이 있는 최적화 문제
(부) 3B1B - Gradient descent, how neural networks learn

### 이번 주는 제대로 이해하지 못해 ㅠㅠ 일단 노트만 남겨본다.

# 최적화란?

- 함수의 값을 가장 크게 or 가장 작게 만드는 입력변수의 값(해/solution)을 찾는 것
- 모든 데이터 분석은 주어진 기준에 따라 적합한 수식을 찾는 것 → 최적화 과정이라고 볼 수 있음
- 일반적으로는 특정한 제한조건(constraint)를 만족하는 해를 찾아야 함
- 최소값에 -1을 곱하면 최대값이 됨
- 최대화/최소화의 대상함수 = 목적함수(objective function)

$$x^{\ast} = \underset{x}{\operatorname{arg\,min}} \; f(x)$$

$$\text{subject to } g(x) \geq 0, \;\;\; h(x) = 0$$

## 그리드 서치와 수치적 최적화

- 그리드 서치(grid search)는 최적화의 가장 간단한 방법
- 가능한 여러개의 독립변수를 넣어보고 그 중 가장 작은 값을 취함
- 모든(많음)위치에 대하여 목적함수를 계산해야 해서 계산량이 상당히 큼
- 그래서 함수 위치가 최적점이 될 때까지 가능한 한 적은 횟수만큼 독립변수의 위치를 옮기는 방법인 수치적 최적화(numerical optimization)이 있음
- 수치적 최적화는 아래 두 가지 알고리즘 요구
  1. 현재 위치가 최적점인지 판단하는 알고리즘
  2. 어떤 위치를 시도한 후 다음번에 시도할 위치를 찾는 알고리즘

## 기울기 필요조건

어떤 독립변수의 값이 최소점이라면 기울기(도함수)가 0이어야 함

단일 변수의 경우 미분값이 0

$$\dfrac{df(x)}{dx} = 0$$

다변수 함수인 경우 모든 변수의 편미분값이 0

$$\dfrac{\partial f(x_1, x_2, \cdots , x_N)}{\partial x_1} = 0$$

$$\dfrac{\partial f(x_1, x_2, \cdots , x_N)}{\partial x_2} = 0 \\ \vdots \\ \dfrac{\partial f(x_1, x_2, \cdots , x_N)}{\partial x_N} = 0$$

즉

$$\nabla f = 0 $$

## 최대경사법

- 현재 위치에서의 기울기만을 이용하여 다음 위치를 결정하는 방법
- 위치를 옮기는 거리를 결정하는 비례상수를 스텝 사이즈(step size)라고 함
- 최적점에 도달한 후는 기울기가 0이므로 더이상 위치를 옮기지 않음

$$x_{k+1} = x_{k} - \mu \nabla f(x_k) = x_{k} - \mu g(x_k)$$

- 스텝 사이즈를 적절히 조정하는 것이 중요

→ 2:40~10:20

https://youtu.be/vsWrXfO3wWw

- 보통 사용자가 경험적으로 얻는 값으로 고정하거나 특정한 알고리즘에 따라 변화
  1. 스텝 사이즈가 너무 작으면 최저점을 찾기까지 시간이 너무 오래 걸림
  2. 스텝 사이즈가 너무 크면 오히려 최저점에서 멀어짐

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6af48494-b084-4e22-b8bc-9f35eef06cd1/FA078B8A-FB09-4FF9-8030-EF433AF22EC2.jpeg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6af48494-b084-4e22-b8bc-9f35eef06cd1/FA078B8A-FB09-4FF9-8030-EF433AF22EC2.jpeg)

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6a44d7f9-6f6e-40b0-9f10-d8a01013d168/B20E13FC-15A7-4473-9B22-2B0BAEFDE3F3.jpeg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6a44d7f9-6f6e-40b0-9f10-d8a01013d168/B20E13FC-15A7-4473-9B22-2B0BAEFDE3F3.jpeg)

- 최적화의 결과는 시작점의 위치나 스텝 사이즈 등에 따라 크게 달라짐

  ![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/181bacef-ce7b-4f3d-a88c-a856433d050d/14F750A3-B190-4AD3-B1F2-CC5344D46F84.jpeg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/181bacef-ce7b-4f3d-a88c-a856433d050d/14F750A3-B190-4AD3-B1F2-CC5344D46F84.jpeg)

  - 그레디언트 벡터가 최저점을 가리키고 있지 않으면 진동 현상이 발생해서 수렴까지 시간이 오래 걸릴 수 있음
  - 2차 도함수(헤시안 행렬)을 이용하거나 모멘텀 방법을 이용해 진동 현상을 없앨 수 있음

## 2차 도함수를 사용한 뉴턴 방법

- 목적함수가 2차 함수라는 가정 하에 한번에 최저점을 찾음
- 그레디언트 벡터에 헤시안 행렬의 역행렬을 곱해서 방향과 거리가 변형된 그레디언트 벡터를 사용

$${x}_{n+1} = {x}_n - [{H}f({x}_n)]^{-1} \nabla f({x}_n)$$

- 스텝 사이즈가 필요없음
- 목적함수가 실제로 2차함수와 비슷한 모양이면 빨리 수렴할 수 있음
- 그러나 그레디언트 벡터(1차 도함수) 뿐만 아니라 2차 도함수(헤시안 행렬)도 필요함

$$f(x) = a(x-x_0)^2 + c = ax^2 -2ax_0x + x_0^2+c$$

$${x}_{n+1} = {x}_n - \dfrac{f'(x_n)}{f''(x_n)}$$

$$f'(x) = 2ax - 2ax_0$$

$$f''(x) = 2a$$

$${x}_{n+1} = {x}_n - \dfrac{2ax_n - 2ax_0}{2a} = x_n - (x_n - x_0) = x_0$$

→ 0:00~3:06 + 5:10~6:20

https://youtu.be/FMCOebUGG94

- 어떤 점에서 시작해도 바로 최저점으로 이동함
- 뉴턴 방법은 헤시안 행렬 함수를 미리 구현해야 하고 함수 모양에 따라서 잘 수렴하지 않을 수 있기 때문에 준 뉴턴(Quasi-Newton) 방법을 사용하기도 함
  - 현재 시도중인 독립변수 값 주변의 점들에서 함수의 값을 구하고, 이를 이용해 2차 도함수의 근삿값을 수치적으로 계산
  - BFGS(Broyden-Fletcher-Goldfarb-Shanno) 방법이 많이 사용 됨
- CG(conjuated gradient) 방법도 헤시안 행렬이 필요하지 않고 변형된 그레디언트 벡터를 바로 계산

## 전역 최적화 문제

- 최적화하려는 함수가 국소 최저점(local minima)를 가지고 있는 경우에는 수치적 최적화 방법으로 전역 최저점(global minimun)에 도달한다는 보장이 없음
- 결과는 초기 추정값 및 알고리즘, 파라미터 등에 의존

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/14d47362-93e0-407f-abe8-319c99103775/F46BFD08-DAF6-4C92-8696-47BCF8FB3B78.jpeg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/14d47362-93e0-407f-abe8-319c99103775/F46BFD08-DAF6-4C92-8696-47BCF8FB3B78.jpeg)

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dac3330a-3fb7-4179-8af1-ae352a25fe0b/532C4EC0-4E86-4398-AB1C-4A77762BAC18.jpeg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dac3330a-3fb7-4179-8af1-ae352a25fe0b/532C4EC0-4E86-4398-AB1C-4A77762BAC18.jpeg)

## 컨벡스 문제

- 목적함수의 2차 도함수의 값이 항상 0이 되는 영역에서만 정의된 최적화 문제를 컨벡스(convex) 문제라고 함

$$\dfrac{\partial^2 f}{\partial x^2} \geq 0$$

→ 0:35~4:00

https://youtu.be/ec2fc2kdd50

- 다변수 목적함수의 경우는 주어진 영역에서의 헤시안 행렬이 항상 양의 준정부호(positive semidefinite)가 됨

$$x^THx \geq 0 \;\;\text{for all } x $$

- 컨벡스 문제에서는 항상 전역 최저점이 존재함

# 제한 조건이 있는 최적화 문제

- 제한조건을 가지는 최적화 문제는 크게 두가지로 나뉘며 다음과 같은 방법으로 품
  1. 연립방정식 → 라그랑주 승수법
  2. 연립부등식 → 라그랑주 승수법 + KKT 조건

## 등식 제한조건

연립방정식 제한 조건이 있는 경우임

$$x^{\ast} = \text{arg} \min_x f(x)$$

$$x \in \mathbf{R}^N $$

$$g_j(x) = 0 \;\; (j=1, \ldots, M) $$

M개의 연립 방정식을 동시에 모두 만족시키면서 목적함수를 가장 작게 하는 독립변수를 찾아야 함

$$g_1(x) = 0 \\ g_2(x) = 0 \\ \vdots \\ g_M(x) = 0$$

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f0f1b197-e8f9-4fc1-bc4a-dea6a64eab98/13DBE4BB-217B-405B-B52A-86E22DD0C3BE.jpeg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f0f1b197-e8f9-4fc1-bc4a-dea6a64eab98/13DBE4BB-217B-405B-B52A-86E22DD0C3BE.jpeg)

## 라그랑주 승수법

- 제한조건 등식에 람다(라그랑주 승수) 라는 새로운 변수를 곱해서 더한 함수를 목적함수(라그랑주 함수)로 간주하여 최적화

$$h(x, \lambda) = h(x_1, x_2, \ldots , x_N, \lambda_1, \ldots , \lambda_M) \\= f(x) + \sum_{j=1}^M \lambda_j g_j(x)$$

- 제한조건 등식 하나마다 새로운 람다가 추가되며, 만약 제한조건이 M개이면 M개의 람다 변수가 생긴 것임
- 결국 새로운 목적함수는 입력변수(람다)가 더 늘어났으므로 그래디언트 벡터를 0으로 만드는 최적화 필요조건은 N+M개가 됨
- N+M개의 연립방정식을 풀면 N+M개의 미지수를 구할수 있으며 그 중 독립변수의 값만 취하면 됨

$$\dfrac{\partial h}{\partial x_1} = \dfrac{\partial f}{\partial x_1} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_1} = 0 \\ \dfrac{\partial h}{\partial x_2} = \dfrac{\partial f}{\partial x_2} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_2} = 0 \\ \vdots \\ \dfrac{\partial h}{\partial x_N} = \dfrac{\partial f}{\partial x_N} + \sum_{j=1}^M \lambda_j\dfrac{\partial g_j}{\partial x_N} = 0 \\ \dfrac{\partial h}{\partial \lambda_1} = g_1 = 0 \\ \vdots \\ \dfrac{\partial h}{\partial \lambda_M} = g_M = 0 $$

$$x_1, x_2, \ldots, x_N, , \lambda_1, \ldots , \lambda_M$$

$$x_1, x_2, \cdots, x_N$$

## 라그랑주 승수의 의미

- 등식 제한조건이 있는가 없는가에 따라 해가 달라지는 경우, 등식 제한조건에 대응하는 라그랑주 승수 람다는 0이 아닌 값이여야 함
- 만약 람다가 0이라면 원래 문제와 제한 조건이 있는 최적화 조건이 같아지므로 최적화 해가 같아져 버리기 때문

## 부등식 제한조건

- 제한조건이 부등식인 경우

$$x^{\ast} = \text{arg} \min_x f(x)$$

$$x \in \mathbf{R}^N$$

$$g_j(x) \leq 0 \;\; (j=1, \ldots, M)$$

$$g_j(x) \geq 0$$

- 부등식이 반대방향이면 양 값에 -1을 곱하면 됨
- 마찬가지로 라그랑주 승수법을 사용하여 목적함수를 바꾸지만, 조건이 달라짐 → KKT(Karush-Kuhn-Tucker) 조건

1. 모든 독립변수에 대한 미분값은 0

→ 라그랑주 승수에 대한 미분이 0이 아니어도 됨

$$\dfrac{\partial h(x, \lambda)}{\partial x_i} = 0$$

1. 모든 라그랑주 승수와 제한조건 부등식(람다에 대한 미분값)의 곱이 0

   a. 라그랑주 승수에 대한 미분이 0

   b. 라그랑주 승수 자체가 0

$$\lambda_j \cdot \dfrac{\partial h(x, \lambda)}{\partial \lambda_j} = \lambda_j \cdot g_j = 0$$

1. 라그랑주 승수는 음수가 아님

$$\lambda_j \geq 0$$

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8a088a78-6bd7-4cd2-91bc-d0aa8b71d554/3FB4468D-0B39-4B0F-AFD6-106EB39479BA.jpeg](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8a088a78-6bd7-4cd2-91bc-d0aa8b71d554/3FB4468D-0B39-4B0F-AFD6-106EB39479BA.jpeg)

- 부등식 제한조건이 있는 최적화 문제는 다음 두 경우 중 하나가 됨
  - 최적화 결과에 영향을 주지 않는 제한조건 → 조건이 없다고 봄
  - 영향을 주는 등식인 제한조건 → 등식 제한조건 문제가 됨
- KKT의 두번째 조건을 다시 상기해보기

$$\lambda^{\ast} = 0 \;\; \text{or} \;\; g(x^{\ast}) = 0 $$

1. a의 경우가 등식 제한조건이 됨(제한조건 유무에 따라 해가 바뀌지 않는 경우를 제외하면 라그랑주 승수는 0이 아님)

$$g_i = 0 \;\; \rightarrow \;\; \lambda_i \neq 0 \; (\lambda_i > 0)$$

1. b의 경우가 제한조건이 의미가 없으므로 라그랑주 승수가 0이 됨

$$g_i \neq 0 \;\; \rightarrow \;\; \lambda_i = 0 $$

- 결국, 부등식 제한조건의 최적화 문제는 각 제한조건에 대한 위의 두 가지 경우를 가정하여 풀면서 값을 찾는 것임