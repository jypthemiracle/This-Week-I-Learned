# 2020.02.05 TIL

## 풀잎스쿨 낯선수학 미적분/최적화 Part 1.

* (주) 4.1~3 함수, 심파이를 사용한 함수 미분, 적분
* (부) 3B1B - Essence of calculus

### Why Calculus?

- 미적분을 배우는 이유

  ![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/204d48a6-8ba6-4bfc-9fb2-a9dbb90ed4ec/_2020-02-05__8.32.30.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/204d48a6-8ba6-4bfc-9fb2-a9dbb90ed4ec/_2020-02-05__8.32.30.png)

> **A lot of hard problems** in math and science **can be broken down and approximated as the sum of many small quantities. -3Blue1Brown**

# 4.1 함수

### 로지스틱 함수

$$\sigma(x) = \dfrac{1}{1 + \exp(-x)} $$

### 로그함수

- log a : 오일러 수 e를 거듭제곱하여 특정한 수 a가 되도록 하는 수

$$y = \log{a}$$

- 로그함수의 성질 1: 곱하기를 더하기로 변환

$$\log{\left(\prod_i x_i\right)} = \sum_i \left(\log{x_i}\right) $$

- 로그함수의 성질 2: 어떤 함수에 로그를 적용해도 함수의 최고점, 최저점의 위치는 변하지 않음

$$\arg\max_x f(x) = \arg\max_x \log f(x)$$

⇒ `**로그 가능도함수(log likelihood function)**`

$$\hat\theta_{\text{ML}} = \arg \max_{\theta} \log{L}(\theta; \{x_i\})$$

### 다변수 다출력 함수

- 입력변수, 출력변수가 복수 개인 함수.

- 입력, 출력 데이터를 벡터나 행렬로 표현함.

- **소프트맥스 함수**

  - 모든 출력 원소는 0~1 사잇값을 갖는다.
  - 모든 출력 원소의 합은 1이다.
  - 입력 원소의 크기 순서와 출력 원소의 크기 순서가 같음.

  **⇒ 다변수 입력을 확률값 처럼 출력.**

  **⇒ 인공신경망 마지막단에서 출력을 조건부확률로 변형하는데 사용.**

$$y = \begin{bmatrix} y_1 \\ y_2 \\ y_3 \end{bmatrix} =S(x_1, x_2, x_3) = \begin{bmatrix} \dfrac{\exp(w_1x_1)}{\exp(w_1x_1) + \exp(w_2x_2) + \exp(w_3x_3)} \\ \dfrac{\exp(w_2x_2)}{\exp(w_1x_1) + \exp(w_2x_2) + \exp(w_3x_3)} \\ \dfrac{\exp(w_3x_3)}{\exp(w_1x_1) + \exp(w_2x_2) + \exp(w_3x_3)} \\ \end{bmatrix}$$

------

# 4.2 심파이를 이용한 함수 미분

### 예측모형의 성능

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d7d1a5e6-9d25-40f7-95dc-d2011a92e239/sm_04_01_.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d7d1a5e6-9d25-40f7-95dc-d2011a92e239/sm_04_01_.png)

- **목적함수(objective function): 최적화의 대상이 되는 함수**
  - 성능함수(performance function)
  - 손실함수(loss function), 비용함수(cost function), 오차함수(error function)
- **최적화(optimization): 목적함수의 값을 최대화 혹은 최소화하는 모수(parameter)를 구하는 것**
- **미분(differentiation): 최적화할 때, 입력값 변화에 따른 출력값 변화를 정량적으로 측정하도록**

### 기울기 (순간변화율 - rate of change)

- 입력값의 변화에 따른 출력값의 변화 정도

$$\dfrac{\Delta y}{\Delta x} = \dfrac{f(x_2) - f(x)}{x_2 - x} = \dfrac{f(x + \Delta x) - f(x)}{\Delta x} $$

$$\text{slope} = \lim_{\Delta x \rightarrow 0} \dfrac{f(x + \Delta x) - f(x)}{\Delta x} $$

### 미분

- **미분(differentiation)**: 어떤 함수의, 그 함수의 기울기를 출력하는 새로운 함수를 만드는 작업
- **도함수(derivative)**: 미분으로 만들어진 함수

$$f' = \dfrac{d}{dx}(f) = \dfrac{d}{dx}f = \dfrac{df}{dx} = \dfrac{d}{dx}(y) = \dfrac{d}{dx}y = \dfrac{dy}{dx} $$

- 함수 변화에 따른 도함수의 변화

https://youtu.be/9vKqVkMQHKk?t=142

- 순간변화율이라는 역설 (3:26~3:42)
- 역설을 극복하는 방법 (7:50~9:00)

https://youtu.be/9vKqVkMQHKk?t=205

- dt를 0으로 수렴하도록 하는 이유 (11:38~13:10)
- hard problem ⇒ small problem (13:52~14:18)

https://youtu.be/9vKqVkMQHKk?t=698

### 기본 미분 공식

- 상수

$$\dfrac{d}{dx}(c) = 0 $$

- 거듭제곱

$$\dfrac{d}{dx}(x^n) = n x^{n-1} $$

- 로그

$$\dfrac{d}{dx}(\log x) = \dfrac{1}{x} $$

- 지수

$$\dfrac{d}{dx}(e^x) = e^x$$

### 선형조합법칙(scaling + sum rule)

- 어떤 함수들을 더하거나, 상수를 곱하거나(스케일링), 상수를 곱한 다음 더해도, 각 함수들의 도함수를 선형조합한 것과 같음.

$$\dfrac{d}{dx}\left(c_1 f_1 + c_2 f_2 \right) = c_1 \dfrac{df_1}{dx} + c_2 \dfrac{df_2}{dx}$$

### 곱셈법칙(product rule)

$$\dfrac{d}{dx}\big( f \cdot g \big) = f \cdot \dfrac{dg}{dx} + \dfrac{df}{dx} \cdot g $$

- 두 함수의 곱의 도함수는, 각 함수의 도함수를 활용하여 구함

$$f = x e^x$$

$$\dfrac{df}{dx} = x e^x + e^x$$

### 연쇄법칙(chain rule)

$$f(x) = h(g(x))$$

- 미분하고자 하는 함수 h(x)의 입력변수 g(x)가, 다른 함수의 출력변수인 경우

$$\dfrac{df}{dx} = \dfrac{dh}{dg} \cdot \dfrac{dg}{dx}$$

### 미분 법칙의 직관적 의미

- Composition, Product, Sum 법칙을 알면 다양한 수식을 미분할 수 있음 (1:18~1:50)
- Sum Rule(1:50~4:10): 요약
- Product Rule(4:32~7:30): dx가 Area(sin(x)*x^2)를 얼마만큼 변화시키는가.
- Chain Rule(8:42~13:15)

https://youtu.be/YG15m2VwSjA?t=78

### 2차 도함수(second derivative)

- 도함수를 한 번 더 미분해서 만든 함수

$$f'' = \dfrac{d^2}{dx^2}(f) = \dfrac{d^2}{dx^2}f = \dfrac{d^2f}{dx^2} = \dfrac{d^2}{dx^2}(y) = \dfrac{d^2}{dx^2}y = \dfrac{d^2y}{dx^2}$$

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/05464d29-a78c-47d1-adfc-51c589b67276/_2020-02-05__8.09.48.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/05464d29-a78c-47d1-adfc-51c589b67276/_2020-02-05__8.09.48.png)

- 양수 ⇒ 볼록(convex)
- 음수 ⇒ 오목(concave)
- 직관적 의미 (4:00~)

https://youtu.be/BLkz5LGWihw?t=240

### 편미분(partial differentiation)

$$f_x(x,y) = \dfrac{\partial f}{\partial x}$$

$$f_y(x,y) = \dfrac{\partial f}{\partial y} $$

- 다변수 함수에서, 하나의 변수에 대해서만 기울기를 구하는 것

### 테일러 전개

$$f(x, y) \approx f(x_0, y_0) + \dfrac{\partial f(x_0, y_0)}{\partial x}(x - x_0) + \dfrac{\partial f(x_0, y_0)}{\partial y}(y - y_0)$$

- 함수의 기울기를 알고 있을 때, 함수의 모양을 다음과 같이 근사하는 것
- y = e^x 예시

https://youtu.be/3d6DsjIBzJ4?t=815

------

# 4.3 적분

### 부정적분(indefinite integral)

$$\dfrac{dF(x)}{dx} = f(x) \;\;\leftrightarrow\;\; F(x) = \int_{}^{} f(x) dx + C $$

- 미분과 반대되는 개념(반 미분)
- 함수 f_x를 도함수로 가정, 미분 전 본래 함수를 찾는 과정(integration) 혹은 결과(integral)
- 부정적분 (11:19~14:56)

https://youtu.be/rfG8ce4nNh0?t=679

### 편미분의 부정적분

- f(x, y)가 F_1(x, y)를 x로 편미분한 경우

  $$\dfrac{\partial F_1(x, y)}{\partial x} = f(x, y) \; \leftrightarrow \; F_1(x, y) = \int_{}^{} f(x, y) dx + C(y)$$

- f(x, y)가 F_1(x, y)를 y로 편미분한 경우

  $$\dfrac{\partial F_2(x, y)}{\partial y} = f(x, y) \; \leftrightarrow \; F_2(x, y) = \int_{}^{} f(x, y) dy + C(x) $$

### 다차 도함수와 다중적분

- 다차 도함수에서 원래 함수를 찾기 위한 작업

  $$\dfrac{\partial^2 F_3(x,y)}{\partial x \partial y} = f(x, y) \; \leftrightarrow \; F_3(x, y) = \int_x \int_y f(x, y) dydx$$

- x로 편미분 → y로 편미분

- y로 적분 → x로 적분

### 정적분

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0b595ca5-f3a8-4d2f-b41c-77e11b317e72/sm_04__.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0b595ca5-f3a8-4d2f-b41c-77e11b317e72/sm_04__.png)

$$\int_{a}^{b} f(x) dx $$

- 특정 구간 [a, b]에서의, 함수 f_(x)값과 수평선(x축)의 면적을 구하는 행위, 그 값
- 정적분은 `**미적분학의 기본 정리**`를 이용하거나, `**수치적분**`을 이용해서 구할 수 있음

### 미적분학의 기본 정리(Fundamental Theorem of Calculus) (14:58~16:18)

$$\int_{a}^{b} f(x) dx = F(b) - F(a) $$

https://youtu.be/rfG8ce4nNh0?t=898

### 수치적분

- 함수 f(x)를 아주 작은 구간으로 나누어, 실제 면적(정적분)을 계산하는 방법
- **위 영상 (9:00~)**

### 다변수 정적분

![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1f22b967-bc41-4fea-9ba4-0acee8c51aeb/sm_04__.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/1f22b967-bc41-4fea-9ba4-0acee8c51aeb/sm_04__.png)

$$\int_{y=c}^{y=d} \int_{x=a}^{x=b} f(x, y) dx dy $$

- 입력변수가 복수 개인 함수에서의 정적분

------

# 4.4 행렬의 미분

- 스칼라 ⇒ 스칼라
- 벡터/행렬 ⇒ 벡터/행렬
- 행렬미분(matrix differentiation)
  - 행렬을 입력, 출력으로 가지는 함수를 (편)미분하는 것

### 스칼라를 벡터로 미분하는 경우

$$\nabla f = \frac{\partial f}{\partial {x}} = \begin{bmatrix} \dfrac{\partial f}{\partial x_1}\\ \dfrac{\partial f}{\partial x_2}\\ \vdots\\ \dfrac{\partial f}{\partial x_N}\\ \end{bmatrix}$$

- 그레디언트 벡터(gradient vector)

### 행렬미분법칙 1 : 선형모형

$$f(x) = w^T x $$

$$\nabla f = \frac{\partial {w}^{T}{x}}{\partial {x}} = \frac{\partial {x}^{T}{w}}{\partial {x}} = {w} $$

- 선형 모형 미분 ⇒ 가중치 벡터(그레디언트 벡터)

### 행렬미분법칙 2 : 이차형식

$$f(x) = x^T A x$$

$$\nabla f(x) = \frac{\partial {x}^{T}{A}{x}}{\partial {x}} = ({A} + {A}^{T}){x}$$

- 이차 형식 미분 ⇒ 행렬과 벡터의 곱

### 벡터를 벡터로 미분하는 경우

$$\dfrac{\partial {f}}{\partial {x}} = \begin{bmatrix} \dfrac{\partial f_1}{\partial {x}} & \dfrac{\partial f_2}{\partial {x}} & \cdots & \dfrac{\partial f_N}{\partial {x}} \end{bmatrix} = \begin{bmatrix} \dfrac{\partial {f}}{\partial x_1} \\ \dfrac{\partial {f}}{\partial x_2} \\ \vdots \\ \dfrac{\partial {f}}{\partial x_M} \end{bmatrix} = \begin{bmatrix} \dfrac{\partial f_1}{\partial x_1} & \dfrac{\partial f_2}{\partial x_1} & \cdots & \dfrac{\partial f_N}{\partial x_1} \\ \dfrac{\partial f_1}{\partial x_2} & \dfrac{\partial f_2}{\partial x_2} & \cdots & \dfrac{\partial f_N}{\partial x_2} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac{\partial f_1}{\partial x_M} & \dfrac{\partial f_2}{\partial x_M} & \cdots & \dfrac{\partial f_N}{\partial x_M} \\ \end{bmatrix}$$

- 미분 당하는 벡터, 미분 하는 벡터의 원소가 복수개.
- 도함수는 행렬이 됨.

### 행렬미분법칙 3 : 행렬과 벡터의 곱의 미분

- Ax를 벡터 x로 미분하면 A^T

- 자코비안 행렬

  $$Jf(x) = J = \left(\frac{\partial f}{\partial x}\right)^T = \begin{bmatrix} \left(\dfrac{\partial f_1}{\partial x}\right)^T \\ \vdots \\ \left(\dfrac{\partial f_M}{\partial x}\right)^T \end{bmatrix} = \begin{bmatrix} \nabla f_1^T \\ \vdots \\ \nabla f_M^T \\ \end{bmatrix} = \begin{bmatrix} \dfrac{\partial f_1}{\partial x_1} & \cdots & \dfrac{\partial f_1}{\partial x_N}\\ \vdots & \ddots & \vdots\\ \dfrac{\partial f_M}{\partial x_1} & \cdots & \dfrac{\partial f_M}{\partial x_N} \end{bmatrix}$$

  - 도함수 행렬
  - 벡터함수를 벡터변수로 미분해서 얻은 행렬의 전치행렬 **(전치행렬인 이유?)**

### 헤시안 행렬(Hessian matrix)

$$H_{ij} = \dfrac{\partial^2 f}{\partial x_i\,\partial x_j}$$

$$H = \begin{bmatrix} \dfrac{\partial^2 f}{\partial x_1^2} & \dfrac{\partial^2 f}{\partial x_1\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_1\,\partial x_N} \\ \dfrac{\partial^2 f}{\partial x_2\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_2^2} & \cdots & \dfrac{\partial^2 f}{\partial x_2\,\partial x_N} \\ \vdots & \vdots & \ddots & \vdots \\ \dfrac{\partial^2 f}{\partial x_N\,\partial x_1} & \dfrac{\partial^2 f}{\partial x_N\,\partial x_2} & \cdots & \dfrac{\partial^2 f}{\partial x_N^2} \end{bmatrix}$$

- 그레디언트 벡터의 자코비안 행렬의 전치 행렬
- 그레디언트 벡터를 입력변수 벡터로 미분한 것 (2차 도함수의 행렬)

> 함수가 연속, 미분가능한 함수. 헤시안 행렬은 **대칭행렬.**

### 행렬미분법칙 4 : 행렬 곱의 대각성분

$$f(X) = \text{tr} ({W}{X})$$

$${W} \in {R}^{N \times N}, {X} \in {R}^{N \times N}$$

$$\dfrac{\partial f}{\partial X} = \dfrac{\partial \, \text{tr} ({W}{X})}{\partial {X}} = {W}^T$$

### 행렬미분법칙 5 : 행렬식의 로그

$$f(X) = \log | {X} |$$

$$\dfrac{\partial f}{\partial X} = \dfrac{\partial \log | {X} | }{\partial {X}} = ({X}^{-1})^T $$

------

# APPENDIX

- 그레디언트 벡터, 자코비안(야코비안) 행렬, 헤시안 행렬을 구하는 이유

https://darkpgmr.tistory.com/132