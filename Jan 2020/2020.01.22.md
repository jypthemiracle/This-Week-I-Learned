# 2020.01.22

* 모두의 연구소 풀잎스쿨 선형대수 2-3주차 학습

# 2.1 데이터와 행렬

### 데이터 유형

- 스칼라: 숫자 하나만으로 이루어진 데이터

$$x \in R$$

- 벡터
  - 여러 숫자가 특정한 순서대로 모여 있는 것
  - 기본적으로 열벡터로 상정
  - *특징벡터: 예측/추론 등 문제에서 입력 데이터로 사용되는 데이터 벡터*

$$x = \begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \\ x_{4} \\ \end{bmatrix}$$

- 행렬(matrix)
  - 복수 차원의 데이터 레코드(벡터)를 합쳐서 표기한 것
  - 행렬로 표시할 때는, 벡터를 열벡터가 아닌 행벡터로 표시
  - 스칼라, 벡터도 수학적으로는 행렬

$$X = \begin{bmatrix} \boxed{\begin{matrix} x_{1, 1} & x_{1, 2} & x_{1, 3} & x_{1, 4}\end{matrix}} \\ \begin{matrix} x_{2, 1} & x_{2, 2} & x_{2, 3} & x_{2, 4}\end{matrix} \\ \begin{matrix} x_{3, 1} & x_{3, 2} & x_{3, 3} & x_{3, 4}\end{matrix} \\ \begin{matrix} x_{4, 1} & x_{4, 2} & x_{4, 3} & x_{4, 4}\end{matrix} \\ \begin{matrix} x_{5, 1} & x_{5, 2} & x_{5, 3} & x_{5, 4}\end{matrix} \\ \begin{matrix} x_{6, 1} & x_{6, 2} & x_{6, 3} & x_{6, 4}\end{matrix} \\ \end{bmatrix}$$

$$X \in \mathbf{R}^{6\times 4} $$

- 텐서(tensor)
  - 같은 크기의 행렬이 여러 개 같이 묶여 있는 것

### 전치 연산

- 가장 기본이 되는 연산
- 행렬의 행과 열을 바꾸는 연산

$$X = \begin{bmatrix} \boxed{\begin{matrix} x_{1, 1} & x_{1, 2} & x_{1, 3} & x_{1, 4}\end{matrix}} \\ \begin{matrix} x_{2, 1} & x_{2, 2} & x_{2, 3} & x_{2, 4}\end{matrix} \\ \begin{matrix} x_{3, 1} & x_{3, 2} & x_{3, 3} & x_{3, 4}\end{matrix} \\ \begin{matrix} x_{4, 1} & x_{4, 2} & x_{4, 3} & x_{4, 4}\end{matrix} \\ \begin{matrix} x_{5, 1} & x_{5, 2} & x_{5, 3} & x_{5, 4}\end{matrix} \\ \begin{matrix} x_{6, 1} & x_{6, 2} & x_{6, 3} & x_{6, 4}\end{matrix} \\ \end{bmatrix} \;\; \rightarrow \;\;$$

$$X^T = \begin{bmatrix} \boxed{\begin{matrix} x_{1, 1} \\ x_{1, 2} \\ x_{1, 3} \\ x_{1, 4}\end{matrix}} & \begin{matrix} x_{2, 1} \\ x_{2, 2} \\ x_{2, 3} \\ x_{2, 4}\end{matrix} & \begin{matrix} x_{3, 1} \\ x_{3, 2} \\ x_{3, 3} \\ x_{3, 4}\end{matrix} & \begin{matrix} x_{4, 1} \\ x_{4, 2} \\ x_{4, 3} \\ x_{4, 4}\end{matrix} & \begin{matrix} x_{5, 1} \\ x_{5, 2} \\ x_{5, 3} \\ x_{5, 4}\end{matrix} & \begin{matrix} x_{6, 1} \\ x_{6, 2} \\ x_{6, 3} \\ x_{6, 4}\end{matrix} & \end{bmatrix}$$

- 넘파이 표현

  A.T

### 행렬의 행 표기법과 열 표기법

- 행렬의 복수의 열벡터 혹은 복수의 행벡터를 합친 형태로 표기 가능

$$X = \begin{bmatrix} c_1 & c_2 & \cdots & c_M \end{bmatrix} = \begin{bmatrix} r_1^T \\ r_2^T \\ \vdots \\ r_N^T \\ \end{bmatrix}$$

### 특수한 벡터와 행렬

- **영벡터: 모든 원소가 0인 N차원 벡터**

$$\mathbf{0}_N = \mathbf{0} = 0 = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 0 \\ \end{bmatrix}$$

- **일벡터: 모든 원소가 1인 N차원 벡터**

$$\mathbf{1}_N = \mathbf{1} = 1 = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \\ \end{bmatrix}$$

- **정방행렬(sqaure matrix): 행 개수와 열 개수가 같은 행렬**

$$X \in \mathbf{R}^{N\times N} $$

$$X = \begin{bmatrix} \boxed{\begin{matrix} x_{1, 1} & x_{1, 2} & x_{1, 3} & x_{1, 4}\end{matrix}} \\ \begin{matrix} x_{2, 1} & x_{2, 2} & x_{2, 3} & x_{2, 4}\end{matrix} \\ \begin{matrix} x_{3, 1} & x_{3, 2} & x_{3, 3} & x_{3, 4}\end{matrix} \\ \begin{matrix} x_{4, 1} & x_{4, 2} & x_{4, 3} & x_{4, 4}\end{matrix} \\ \begin{matrix} \end{matrix} \\ \end{bmatrix} \;\; $$

- 대각행렬(diagonal matrix)
  - 모든 비대각 요소가 0인 행렬
  - 반드시 정방행렬일 필요는 없음

$$D = \begin{bmatrix} d_{1} & 0 & \cdots & 0 \\ 0 & d_{2} & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & d_{M} \\ 0 & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ 0 & 0 & \cdots & 0 \\ \end{bmatrix}$$

- **항등행렬(identity matrix): 모든 대각 요소가 1인 행렬**

$$I = \begin{bmatrix} 1 & 0 & \cdots & 0 \\ 0 & 1 & \cdots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \cdots & 1 \\ \end{bmatrix}$$

- **대칭행렬(symmetric matrix): 전치연산으로 얻은 전치행렬과 원래 행렬이 같은 행렬**

$$S = \begin{bmatrix} 1 & 2 & \cdots & 5 \\ 2 & 1 & \cdots & 4 \\ \vdots & \vdots & \ddots & \vdots \\ 5 & 4 & \cdots & 1 \\ \end{bmatrix}$$

------

# 2.2 벡터와 행렬의 연산

### 벡터/행렬의 덧셈과 뺄셈

- 요소 별 연산(element-wise)

$$x + y = \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} + \begin{bmatrix} 0 \\ 1 \\ 2 \\ \end{bmatrix} = \begin{bmatrix} 10 + 0 \\ 11 + 1 \\ 12 + 2 \\ \end{bmatrix} = \begin{bmatrix} 10 \\ 12 \\ 14 \\ \end{bmatrix}$$

### 스칼라와 벡터/행렬의 곱셈

- 벡터, 행렬의 모든 원소에 스칼라 c를 곱하는 것

$$c \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} = \begin{bmatrix} ca_{11} & ca_{12} \\ ca_{21} & ca_{22} \end{bmatrix}$$

### 브로드캐스팅

- 벡터와 스칼라의 경우, 관례적으로 1-벡터를 사용

$$\begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} - 10 = \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} - 10\cdot \mathbf{1} = \begin{bmatrix} 10 \\ 11 \\ 12 \\ \end{bmatrix} - \begin{bmatrix} 10 \\ 10 \\ 10 \\ \end{bmatrix}$$

### 선형조합(linear combination) *

- 벡터/행렬에 다음처럼 스칼라 값을 곱한 후, 더하거나 뺀 것

$$c_1x_1 + c_2x_2 + c_3x_3 + \cdots + c_Lx_L = x$$

https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7f675f0d-da2d-48cf-98c6-f562a942319c/sm_02_linear_combination.mov

### 벡터와 벡터의 곱셈 *

- **내적(inner product / dot product): 벡터와 벡터의 곱**
- 조건
  - 두 벡터의 차원(길이)가 같아야 함
  - 앞 벡터가 행벡터이고 뒤 벡터가 열벡터야 함

$$x^Ty$$

$$x^T y = \begin{bmatrix} x_{1} & x_{2} & \cdots & x_{N} \end{bmatrix} \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{N} \\ \end{bmatrix} = x_1 y_1 + \cdots + x_N y_N = \sum_{i=1}^N x_i y_i$$

https://s3-us-west-2.amazonaws.com/secure.notion-static.com/71da4848-da36-4f33-84e3-793a541fa053/sm_02_inner_product.mov

### 가중합/가중평균

- 가중합: 복수의 데이터를, 가중치를 곱한 후 합하는 것
- 가중평균: 가중합의 가중치값을 전체 가중치값의 합으로 나누어 구한 값

$$w_1 x_1 + \cdots + w_N x_N = \sum_{i=1}^N w_i x_i$$

### 유사도 *

- 두 벡터가 닮은 정도를 정량적으로 표현한 값
- 내적을 이용하여 코사인 유사도를 계산할 수 있음

### 행렬과 행렬의 곱셈

- 교환 법칙과 분배 법칙: 교환 법칙은 성립하지 않으나, 덧셈에 대한 분배 법칙은 성립함
- (행렬) 곱셈의 연결: 연속된 행렬의 곱셈은 계산 순서를 바꿔도 상관없음

### 선형회귀 모형(linear regression model) *

- 독립변수 x에서 종속변수 y를 예측하는 방법의 하나
- 독립변수 벡터 x와 가중치 벡터 w의 가중합으로 y에 관한 예측값 y-hat을 계산

$$\hat{y} = w_1 x_1 + \cdots + w_N x_N$$

- **비선형적 문제를 잘 예측하지 못할 수 있음**

### 제곱합(sum of squares)

- 각 데이터(벡터)를 제곱하여 모두 더한 값

$$x^T x = \begin{bmatrix} x_{1} & x_{2} & \cdots & x_{N} \end{bmatrix} \begin{bmatrix} x_{1} \\ x_{2} \\ \vdots \\ x_{N} \\ \end{bmatrix} = \sum_{i=1}^{N} x_i^2 $$

### 잔차(residual, error) *

- 예측치와 실젯값 사이의 차이

$$\begin{aligned} e &= \begin{bmatrix} e_{1} \\ e_{2} \\ \vdots \\ e_{M} \\ \end{bmatrix} \\ &= \begin{bmatrix} y_{1} \\ y_{2} \\ \vdots \\ y_{M} \\ \end{bmatrix} - \begin{bmatrix} x^T_{1}w \\ x^T_{2}w \\ \vdots \\ x^T_{M}w \\ \end{bmatrix} \\ &= y - Xw \end{aligned}$$

- **잔차 제곱합(Residual Sum of Squares): 잔차벡터의 각 원소를 제곱한 후 더한 값**

$$\sum_{i=1}^{N} e_i^2 = \sum_{i=1}^{N} (y_i - w^Tx_i)^2 = e^Te = (y - Xw)^T (y - Xw)$$

### 이차형식(Quadratic Form) *

- 어떤 벡터와 정방행렬이 `행벡터 x 정방행렬 x 열벡터` 형태로 되어 있는 것

$$w^TX^TXw$$

# 2.3 행렬의 성질

- 행렬의 부호
- 행렬의 크기

### 정부호와 준정부호

- 양의 정부호(positive definite): 영벡터가 아닌 모든 벡터 x에 대해 아래 부등식이 성립하면, 행렬 A는 양의 정부호.

$$x^T A x > 0 $$

- 양의 준정부호: 아래 부등식이 등호를 포함한다면, 행렬 A는 양의 준정부호.

$$x^T A x \geq 0 $$

### 행렬 놈(norm) *

- 행렬(벡터)의 크기를 나타내는 값

$$\Vert A \Vert_p = \left( \sum_{i=1}^N \sum_{j=1}^M |a_{ij}|^p \right)^{1/p}$$

- p는 1, 2 혹은 무한대.
- 일반적으로 p = 2인 경우가 가장 많이 쓰임.
- 놈의 공식적 정의 4가지

### 대각합(trace) *

- 대각원소의 합. 정방행렬에 대해서만 정의됨.

- 대각합의 성질

  - 1 ~ 3.

  - **4. 두 행렬의 곱의 대각합은 행렬의 순서를 바꿔도 달라지지 않음.**

  - **5. 세 행렬의 곱의 대각합은 순서를 순환해도 달라지지 않음.**

    **→ 트레이스 트릭(trace trick)**

$$x^TAx = \text{tr}(x^TAx) = \text{tr}(Axx^T) = \text{tr}(xx^TA) $$

### 행렬식(determinant) *

- 행렬식의 기하학적 의미

https://s3-us-west-2.amazonaws.com/secure.notion-static.com/258b396d-008c-40b9-bc85-bbd8cf5cd874/sm_02_deteminant.mov

- **행렬식의 성질**
  1. 전치행렬의 행렬식 = 본래 행렬의 행렬식 `det(A.T) = det(A)`
  2. 항등행렬의 행렬식 = 1 `det(I) = 1`
  3. 두 행렬식의 곱 = 각 행렬의 행렬식의 곱 `det(AB) = det(A) * det(B)`
  4. 역행렬의 행렬식 = 본래 행렬의 행렬식의 역수 `det(A^-1) = 1 / det(A)`
- **행렬식을 구하는 방법: 여인수 전개(cofactor expansion)으로 계산 - 재귀적 방법**

------

# 2.4 선형 연립방정식과 역행렬

### 선형 연립방정식 *

- 복수의 미지수를 포함하는 복수의 선형 방정식

$$\begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1M} \\ a_{21} & a_{22} & \cdots & a_{2M} \\ \vdots & \vdots & \ddots & \vdots \\ a_{N1} & a_{N2} & \cdots & a_{NM} \\ \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_M \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_N \end{bmatrix}$$

$$A = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1M} \\ a_{21} & a_{22} & \cdots & a_{2M} \\ \vdots & \vdots & \ddots & \vdots \\ a_{N1} & a_{N2} & \cdots & a_{NM} \\ \end{bmatrix} , \;\; x = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_M \end{bmatrix} , \;\; b= \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_N \end{bmatrix}$$

https://s3-us-west-2.amazonaws.com/secure.notion-static.com/908836d5-b58e-4614-a510-621862799e5c/sm_02_04_linear_equation_system.mov

- A: 계수 행렬(coefficient matrix)
- x: 미지수벡터(unknown vector)
- b: 상수 벡터(constant vector)

### 역행렬 *

$$A^{-1} A = A A^{-1} = I $$

- 역행렬의 기하학적 의미
  - det(A) ≠ 0

https://s3-us-west-2.amazonaws.com/secure.notion-static.com/43334cbc-33a3-4ff4-95ad-e333464e7e09/sm_02_05_inverse_transformation_1.mov

- det(A) = 0

https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bdf4e60b-cebe-41d9-aa3f-3aab9e36c0be/sm_02_06_inverse_transformation_2.mov

- **항상 존재하지는 않음.**
  - 존재하는 경우: 가역행렬, 정칙행렬, 비특이행렬
  - 존재하지 않는 경우: 비가역행렬, 특이행렬, 퇴화행렬
- 역행렬의 성질
  1. 전치행렬의 역행렬 = 역행렬의 전치행렬 `(A.T)^-1 = (A^-1).T`
  2. 행렬의 곱의 역행렬. `(AB)^-1 = B^-1 * A^-1`
- 역행렬의 계산

### 역행렬과 선형연립방정식의 해

- 행렬 A의 역행렬이 존재한다면, 역행렬의 정의를 이용해 선형 연립방정식의 해를 구할 수 있음

$$Ax = b$$

$$A^{-1}Ax = A^{-1}b$$

$$Ix = A^{-1}b$$

$$x = A^{-1}b $$

### 미지수 수와 방정식 수

- N = M: 선형 연립방정식 `if det(A) ≠ 0`
- N < M: 무수히 많은 해
- **N > M: 해 x → 최소자승문제**

### 최소자승문제(least square problem)

- 잔차를 최소화하는 미지수 벡터 x(보통 w벡터 - weight)를 구함

$$e = Ax - b$$

$$e^Te = (Ax-b)^T(Ax-b)$$

$$x = \text{arg} \min_x e^Te = \text{arg} \min_x \; (Ax-b)^T(Ax-b) $$

- **미지수 벡터를 구하는 방법: 의사역행렬(pseudo inverse)**

$$A^TAx = A^Tb $$

$$(A^TA)^{-1}(A^TA)x = (A^TA)^{-1}A^Tb $$

$$x = ((A^TA)^{-1}A^T) b $$

$$A^{+} = (A^TA)^{-1}A^T $$

- [**최소자승문제(최소제곱법)에 대한 추가 내용**](https://www.youtube.com/watch?v=lBSwTwcQwuY&list=PLSN_PltQeOyjDGSghAf92VhdMBeaLZWR3&index=12&t=0s)

  $$\{ x_1, x_2, \ldots, x_N \} \tag{1.2.6}$$

### 벡터의 기하학적 의미

- n차원 벡터 a는 n차원 공간의 **점(point)** 혹은 **화살표(arrow)**

  - **CS: Tuple**
  - **Mathematician: +a**

  https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6c1b1921-f111-4c55-b662-d65c8e22a865/03_00_vector.mov

  https://s3-us-west-2.amazonaws.com/secure.notion-static.com/020e5bef-5dea-4b60-9177-979aaca4cb09/03_00_vector_02.mov

### 벡터의 합, 스케일링, 선형조합

- 벡터의 합

  https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fff06a69-059e-460f-b38a-953a4c73ef10/sm_03_01_add_vector.mov

- 스칼라와 벡터의 곱

  https://s3-us-west-2.amazonaws.com/secure.notion-static.com/0ce5bb1a-521c-4cb6-878e-8bbf0d3a95d3/sm_03_02_scaling.mov

### (벡터의 차)

- b + (a - b) = a

  ![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/73835532-91ae-4cf8-9cdc-e99820774f45/_2020-01-22__8.15.22.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/73835532-91ae-4cf8-9cdc-e99820774f45/_2020-01-22__8.15.22.png)

### 유클리드 거리, 코사인 유사도(거리)

- **유클리드 거리**

  $$\begin{aligned} \| a - b \| &= \sqrt{\sum_{i=1} (a_i - b_i)^2} \\ &= \sqrt{\sum_{i=1} ( a_i^2 - 2 a_i b_i + b_i^2 )} \\ &= \sqrt{\sum_{i=1} a_i^2 + \sum_{i=1} b_i^2 - 2 \sum_{i=1} a_i b_i} \\ &= \sqrt{\| a \|^2 + \| b \|^2 - 2 a^Tb } \end{aligned}$$

  - **두 벡터가 가리키는 점 사이의 거리**
  - 벡터 차의 길이**(norm)**으로 구할 수 있음

* 코사인 유사도

$$a^Tb = \|a\|\|b\| \cos\theta$$

$$\text{cosine similarity} = \cos\theta = \dfrac{x^Ty}{\|x\|\|y\|}$$

- 두 벡터의 **방향(각도)** 사이의 값

  $$-1 \leq \cos\theta \leq 1$$

$$-1 \leq \dfrac{x^Ty}{\|x\|\|y\|} \leq 1$$

- **코사인 거리**

  $$\text{cosine distance} = 1 - \text{cosine similarity} = 1 - \dfrac{x^Ty}{\|x\|\|y\|}$$

### 단위벡터, 직교, 정규직교

- **단위벡터**(unit vector)**: 길이가 1인 벡터**

  $$a = \begin{bmatrix}1 \\ 0\end{bmatrix} ,\;\; b = \begin{bmatrix}0 \\ 1\end{bmatrix} ,\;\; c = \begin{bmatrix} \dfrac{1}{\sqrt{2}} \\ \dfrac{1}{\sqrt{2}} \end{bmatrix}$$

- **직교**(orthogonal)**: 두 벡터 a와 b가 이루는 각이 90도**

  $$a^T b = b^T a = 0 \;\;\;\; \leftrightarrow \;\;\;\; a \perp b $$

### 벡터의 분해, 투영성분과 직교성분

- 분해(decomposition): 두 벡터 A, B의 합이 다른 벡터 C가 될때. **(곱은?)**

  $$a = a^{\perp b} + a^{\Vert b} $$

  ![https://s3-us-west-2.amazonaws.com/secure.notion-static.com/63349c6c-140f-4695-bfd4-d32b35ac76b8/_2020-01-22__8.26.53.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/63349c6c-140f-4695-bfd4-d32b35ac76b8/_2020-01-22__8.26.53.png)

- **투영성분(projection)**: 벡터 a를 벡터 b에 직교하는 성분, 평행하는 성분으로 **분해했을 때 평행하는 성분**

  - 벡터의 길이
  - 벡터의 방향
  - `{길이}` * `{방향}`

  $$a^{\Vert b} $$

  $$\cos\theta = \dfrac{\| a^{\Vert b} \|}{\|a\|}$$

$$\| a^{\Vert b} \| = \|a\|\cos\theta = \dfrac{\|a\|\|b\|\cos\theta}{\|b\|} = \dfrac{a^Tb}{\|b\|} = \dfrac{b^Ta}{\|b\|} = a^T\dfrac{b}{\|b\|}$$

$$a^{\Vert b} = \dfrac{a^Tb}{\|b\|} \dfrac{b}{\|b\|}= \dfrac{a^Tb}{\|b\|^2}b $$

- **직교성분(rejection)**: 벡터 a를 벡터 b에 직교하는 성분, 평행하는 성분으로 **분해했을 때 직교하는 성분**

  $$a^{\perp b} = a - a^{\Vert b} $$

# 3.2 좌표와 변환

### 기저벡터, 표준기저벡터

- **기저벡터(basis vector)**

  - 열공간(columns space)을 선형조합으로 구성하는 벡터

  https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d7f71897-f066-4cfc-b742-bbb0f9676ef5/sm_03_03_basis_vector.mov

- **표준기저벡터(standard basis vector)**

  - 기저벡터 중 하나
  - 원소 중 하나만 값이 1개이고 다른 값은 0개로 이루어진 기저벡터

  $$e_1 = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \;\; e_2 = \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \;\; \cdots, \;\; e_N = \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix}$$